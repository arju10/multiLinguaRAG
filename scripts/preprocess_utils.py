# python scripts/preprocess_utils.py
import unicodedata
import re
import unicodedata
from pymongo import MongoClient
from dotenv import load_dotenv
import os
import regex
import regex as re

load_dotenv()

client = MongoClient(os.getenv("MONGO_URI"))
db = client["multilinguarag"]
collection = db["chunks"]


def normalize_unicode(text: str) -> str:
    # Normalizing Bangla chars
    return unicodedata.normalize('NFKC', text)

def remove_unwanted_chars(text: str) -> str:
    # Keep only Bangla, English, numbers, some punctuations
    pattern = r"[^‡¶Ä-‡ßøa-zA-Z0-9.,?!‡•§:;'\"()\-‚Äì‚Äî\s]"
    return re.sub(pattern, "", text)

def clean_whitespace(text: str) -> str:
    text = text.replace("\n", " ").replace("\t", " ")
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def normalize_punctuation(text: str) -> str:
    # Multiple full stops --> single
    text = re.sub(r"‡•§+", "‡•§", text)
    text = re.sub(r"\.\.+", ".", text)
    return text

def fix_bangla_ocr_errors(text: str) -> str:
    # Common OCR errors in Bangla
    replacements = {
        '‡¶Ö‡¶™‡¶∞‡¶ø‡¶∞‡¶ø‡¶§‡¶æ': '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ',
        '‡¶Ö‡¶™‡¶∞‡ßç‡¶ø‡¶∞‡ßç‡¶ö‡¶§‡¶æ': '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ',
        '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ': '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ',  # Normalize variants
        '‡¶ï‡¶≤‡¶ø‡¶æ‡•§‡¶ò‡•§': '‡¶ï. ‡¶ò.',
        '‡¶ï‡¶≤‡¶Ø‡¶æ‡¶£‡ßÄ': '‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ',
        '‡¶Æ‡¶æ‡¶Æ‡¶æ': '‡¶Æ‡¶æ‡¶Æ‡¶æ',
        '‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ': '‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ',
        '‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•': '‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•',
        '‡¶¨‡ßç‡¶ï': '‡¶¨',       # Fix broken conjuncts
        '‡ßç‡¶ï': '‡ßç',         # Fix misplaced halant
        '‡¶ø': '‡¶ø',          # Fix vowel signs
        '‡ßá': '‡ßá',
        '‡ßã': '‡ßã',
        '‡ßÄ': '‡ßÄ',
        '‡ßÅ': '‡ßÅ',
        '‡ßÇ': '‡ßÇ',
        '‡¶Ç': '‡¶Ç',
        '‡¶É': '‡¶É',
        '‡¶Å': '‡¶Å',
        '‡¶¶‡ßç': '‡¶¶',
        '‡¶®‡ßç': '‡¶®',
        '‡¶Æ‡ßç': '‡¶Æ',
        '‡¶™‡ßç': '‡¶™',
        '‡¶¨‡ßç': '‡¶¨',
        '‡¶≠‡ßç': '‡¶≠',
        '‡¶∞‡ßç': '‡¶∞',
        '‡¶∑‡ßç': '‡¶∑',
        '‡¶∏‡ßç': '‡¶∏',
        '‡¶§‡ßç': '‡¶§',
        '‡¶ü‡ßç': '‡¶ü',
        '‡¶°‡ßç': '‡¶°',
        '‡¶ß‡ßç': '‡¶ß',
        '‡¶≤‡ßç': '‡¶≤',
        '‡¶ï‡ßç': '‡¶ï',
        '‡¶ó‡ßç': '‡¶ó',
        '‡¶ò‡ßç': '‡¶ò',
        '‡¶ö‡ßç': '‡¶ö',
        '‡¶õ‡ßç': '‡¶õ',
        '‡¶ú‡ßç': '‡¶ú',
        '‡¶ù‡ßç': '‡¶ù',
        '‡¶û‡ßç': '‡¶û',
        '‡¶¶‡ßç‡¶¨': '‡¶¶‡ßç‡¶¨',
        '‡¶§‡ßç‡¶∞': '‡¶§‡ßç‡¶∞',
        '‡¶ú‡ßç‡¶û': '‡¶ú‡ßç‡¶û',
        '‡¶ï‡ßç‡¶∑': '‡¶ï‡ßç‡¶∑',
        '‡¶∂‡ßç‡¶∞': '‡¶∂‡ßç‡¶∞',
    }
    for wrong, correct in replacements.items():
        text = text.replace(wrong, correct)
    return text

# Remove Page Numbers, Headers, and Noise Lines
def remove_noise_lines(text: str) -> str:
    lines = text.split('\n')
    cleaned_lines = []
    noise_patterns = [
        r'^\s*P[AGE]+\s*\d+',            # PAGE 24
        r'^\s*\?+\s*$',                   # ?????
        r'^\s*SLAns.*$',                  # Answer key lines
        r'^\s*[i|ii|iii|iv]+\s*\.\s*$',   # Bullet points
        r'^\s*‡¶â‡¶§‡ßç‡¶§‡¶∞\s*:',                 # Answer headers
        r'^\s*‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®\-\s*\d+',            # "‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®- ‡ß©"
        r'^\s*\[\w+\.?\s*‡¶¶‡¶ø‡¶æ\.?\s*\'?\d+', # [‡¶¢‡¶æ. ‡¶¶‡¶ø‡¶æ. '‡ß®‡ß®]
    ]
    
    for line in lines:
        if len(line.strip()) < 10:  # Too short
            continue
        if any(re.match(pattern, line.strip()) for pattern in noise_patterns):
            continue
        cleaned_lines.append(line.strip())
    
    return '\n'.join(cleaned_lines)

# Fix Broken Words (Conjuncts & Vowel Signs)
def repair_bangla_conjuncts(text: str) -> str:
    # Fix common broken conjuncts
    text = regex.sub(r'([‡¶ï-‡¶π])\s+‡ßç\s+([‡¶ï-‡¶π])', r'\1‡ßç‡¶Ø\2', text)  # e.g., ‡¶ï ‡ßç ‡¶∑ ‚Üí ‡¶ï‡ßç‡¶∑
    text = regex.sub(r'([‡¶ï-‡¶π])\s+‡¶ø', r'\1‡¶ø', text)  # fix spacing around vowel signs
    text = regex.sub(r'([‡¶ï-‡¶π])\s+‡ßÄ', r'\1‡ßÄ', text)
    return text



def remove_garbage_tokens(text: str) -> str:
    # Word tokenize: Bangla, English, Numbers, Punctuation separate
    words = re.findall(r'\w+|[^\s\w]', text, re.UNICODE)

    cleaned_words = []
    for w in words:
        # Only digit or Bengali digit or colon like 1081:39:
        if re.fullmatch(r"[0-9‡ß¶-‡ßØ:]+", w):
            continue
        # Single Bangla letter like ‡¶ï ‡¶ñ ‡¶ó ‡¶ò
        if re.fullmatch(r"[‡¶Ö-‡¶î‡¶ï-‡¶π]", w):
            continue
        # Standalone punctuation () ‡¶¨‡¶æ single dot or single danda
        # if w in {"(", ")", "‡•§", ".", ",", ":", ";", "?", "!", "-", "‚Äì", "‚Äî"}:
        if w in {"(", ")", "‡•§", ".", ",", ":", ";", "?", "!", "-", "‚Äì", "‚Äî", "'", '"'}:
            continue
        cleaned_words.append(w)

    return " ".join(cleaned_words)


# Reconstruct Structure: Identify Questions, Answers, Themes
def extract_thematic_sections(text: str) -> dict:
    sections = {
        "questions": [],
        "answers": [],
        "character_analysis": [],
        "themes": []
    }

    # Extract MCQs
    mcq_pattern = r'(\d+‡•§\s*[^‡•§]+‡•§)\s*\[.*?\]\s*\((‡¶ï|‡¶ñ|‡¶ó|‡¶ò)\)\s*‡¶â‡¶§‡ßç‡¶§‡¶∞:\s*(‡¶ï|‡¶ñ|‡¶ó|‡¶ò)'
    for match in regex.finditer(mcq_pattern, text):
        sections["questions"].append(match.group(1))

    # Extract answers
    answer_pattern = r'‡¶â‡¶§‡ßç‡¶§‡¶∞:\s*(‡¶ï|‡¶ñ|‡¶ó|‡¶ò)'
    for match in regex.finditer(answer_pattern, text):
        sections["answers"].append(match.group(1))

    # Extract character analysis
    char_patterns = ['‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ', '‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ', '‡¶Æ‡¶æ‡¶Æ‡¶æ', '‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•', '‡¶Æ‡¶æ']
    for char in char_patterns:
        context = regex.findall(rf'(.{{0,100}}{char}[^‡•§]*‡•§{{1,3}}[^‡•§]*‡•§)', text)
        sections["character_analysis"].extend(context)

    # Extract themes
    theme_keywords = ['‡¶Æ‡¶æ‡¶§‡ßÉ‡¶∏‡ßç‡¶®‡ßá‡¶π', '‡¶Ø‡ßå‡¶§‡ßÅ‡¶ï', '‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßç‡¶¨‡¶π‡ßÄ‡¶®‡¶§‡¶æ', '‡¶∏‡ßç‡¶¨‡¶æ‡¶ß‡ßÄ‡¶®‡¶§‡¶æ', '‡¶Ö‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡¶æ']
    for keyword in theme_keywords:
        context = regex.findall(rf'(.{{0,150}}{keyword}[^‡•§]*‡•§{{1,3}}[^‡•§]*‡•§)', text)
        sections["themes"].extend(context)

    return sections



def semantic_chunking(text: str) -> list:
    chunks = []

    # Try splitting by double newline (paragraphs)
    paragraphs = re.split(r'\n\n+', text)
    print(f"üîç Found {len(paragraphs)} paragraphs.")

    for para in paragraphs:
        para = para.strip()
        if len(para) < 50:  
            continue

        if len(para) <= 300:  
            chunks.append(para)
        else:
            # Split by full stops, newlines, punctuation
            sentences = re.split(r'[‡•§.!?\n]', para)
            current_chunk = ""
            for sent in sentences:
                sent = sent.strip()
                if len(sent) < 10:
                    continue
                if len(current_chunk) + len(sent) > 300:
                    chunks.append(current_chunk.strip())
                    current_chunk = sent
                else:
                    current_chunk += " " + sent
            if current_chunk:
                chunks.append(current_chunk.strip())

    print(f"‚úÖ Chunks after normal split: {len(chunks)}")

    # If still too few, fallback
    if len(chunks) <= 2:
        print("‚ö†Ô∏è Too few chunks. Forcing fallback chunking...")
        forced_chunks = []
        forced_text = text.strip()
        chunk_size = 300
        for i in range(0, len(forced_text), chunk_size):
            piece = forced_text[i:i+chunk_size].strip()
            if len(piece) > 50:
                forced_chunks.append(piece)
        chunks = forced_chunks
        print(f"‚úÖ Fallback forced chunks: {len(chunks)}")

    return chunks





# Final Enhanced Preprocessing Function
def pre_process_enhanced(text: str) -> dict:
    text = normalize_unicode(text)
    text = fix_bangla_ocr_errors(text)
    text = remove_unwanted_chars(text)
    text = repair_bangla_conjuncts(text)
    text = clean_whitespace(text)
    text = normalize_punctuation(text)
    text = remove_noise_lines(text)
    text = remove_garbage_tokens(text) 
    chunks = semantic_chunking(text)

    # Extract structured content
    structured = extract_thematic_sections(text)

    # Create semantic chunks

    return {
        "cleaned_text": text,
        "chunks": chunks,
        "structured": structured
    }


# --- Run directly ---
doc = collection.find_one({"book_name": "HSC26 Bangla 1st Paper"})
raw = doc["content"]
print(f"Raw Length: {len(raw)}")

result = pre_process_enhanced(raw)
cleaned = result["cleaned_text"]
chunks = result["chunks"]
sections = result["structured"]

print(f"Cleaned Length: {len(cleaned)} | Chunks: {len(chunks)}")

# Save to MongoDB
collection.update_one(
    {"_id": doc["_id"]},
    {"$set": {
        "cleaned_content": cleaned,
        "semantic_chunks": chunks,
        "sections": sections
    }}
)
print("‚úÖ Saved cleaned, chunks & sections to MongoDB!")

# Save locally
os.makedirs("./data", exist_ok=True)
with open("./data/cleaned_text_HSC26.txt", "w", encoding="utf-8") as f:
    f.write(cleaned)
with open("./data/chunks_HSC26.txt", "w", encoding="utf-8") as f:
    for chunk in chunks:
        f.write(chunk + "\n---\n")
print("‚úÖ Saved cleaned & chunks locally!")
